{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff2dadaf-d91c-453f-a29f-2f30a4592627",
   "metadata": {},
   "source": [
    "## Model to Identify Plant Leaves\n",
    "\n",
    "## Abstract\n",
    "This project focuses on developing an automated system for identifying various species of leaves using the Xception model, a deep learning architecture known for its efficiency in image classification tasks. The model is trained on a diverse dataset of plant leaves to enhance accuracy and reliability.\n",
    "\n",
    "## Introduction\n",
    "With the increasing interest in plant biodiversity, the need for automated identification systems has become paramount. This project aims to leverage advanced machine learning techniques to accurately classify plant leaves based on image data.\n",
    "\n",
    "## Project Overview\n",
    "- **Objective**: To create a robust system capable of identifying and classifying leaf species from images.\n",
    "- **Dataset**: The dataset comprises images of 83 different leaf species, totaling over 7,000 images.\n",
    "- **Model Architecture**: The Xception model is utilized due to its superior performance in handling image data through depthwise separable convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1007688d-d94a-48b9-b3f6-6f9d5a546a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b055dc-6361-4146-b37f-76343cce0d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12845 files belonging to 2 classes.\n",
      "['Medicinal Leaf dataset', 'Medicinal plant dataset']\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"C:\\\\Users\\\\wwwja\\\\Downloads\\\\Indian Medicinal Leaves Image Datasets\\\\Indian Medicinal Leaves Image Datasets\",\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    image_size=(299, 299),\n",
    ")\n",
    "\n",
    "labels = dataset.class_names\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390f99d2",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "- The dataset consists of images sourced from various botanical collections.\n",
    "- Each class corresponds to a specific leaf species, facilitating supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6e115dc-5de0-4e13-9509-0652ff5f1c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 299, 299, 3)\n",
      "[1 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for image_batch, labels_batch in dataset.take(1):\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c956dd55-e52c-4a7c-862c-357640a04b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(321, 80)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train test split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = int(0.2 * len(dataset))\n",
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70d6cf17-814e-41ce-8e58-78fe2bfe6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_partisions_tf(ds, train_split=0.8, test_split=0.2, shuffle=True, shuffle_size=10000):\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    train_size = int(train_split * len(ds))\n",
    "    test_size = int(test_split * len(ds))\n",
    "    train_ds = ds.take(train_size)\n",
    "    test_ds = ds.skip(train_size)\n",
    "    val_ds = test_ds.skip(test_size)\n",
    "    test_ds = test_ds.take(test_size)\n",
    "    return train_ds, test_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abe21acf-7850-4635-8c91-27c94a088045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(321, 80, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, test_ds, val_ds = get_dataset_partisions_tf(dataset)\n",
    "len(train_ds), len(test_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46142013-0e00-41b6-b10b-2592628c6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(299, 299),\n",
    "    tf.keras.layers.Rescaling(1./255)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a34fa0ce-87eb-4174-8686-46ab7841819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using Xception\n",
    "base_model = tf.keras.applications.Xception(\n",
    "    weights='imagenet',\n",
    "    input_shape=(299, 299, 3),\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    classifier_activation='softmax',\n",
    "    classes=len(labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12369120",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "### Xception Model Overview\n",
    "The Xception model is an advanced deep learning architecture based on the Inception model, characterized by its use of depthwise separable convolutions, which improve model performance while reducing computational costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb1dd7-1bf9-43c5-8011-567c40951c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ xception (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │      <span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m299\u001b[0m, \u001b[38;5;34m299\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ sequential (\u001b[38;5;33mSequential\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m299\u001b[0m, \u001b[38;5;34m299\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ xception (\u001b[38;5;33mFunctional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │      \u001b[38;5;34m20,861,480\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m262,272\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m258\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,124,010</span> (80.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,124,010\u001b[0m (80.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">262,530</span> (1.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m262,530\u001b[0m (1.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> (79.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m20,861,480\u001b[0m (79.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    }
   ],
   "source": [
    "base_model.trainable = False\n",
    "\n",
    "inputs = tf.keras.Input(shape=(299, 299, 3))\n",
    "x = resize_and_rescale(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(len(labels), activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    batch_size=32,\n",
    "    epochs=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c7e337-a4e1-4860-a2fb-1a707a910720",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8573c7b8",
   "metadata": {},
   "source": [
    "## Testing the Model with Known Data\n",
    "\n",
    "### Objective\n",
    "The purpose of this section is to evaluate the performance of the trained Xception model on a test dataset containing images of leaves whose species are already known. This evaluation helps assess the model's accuracy and reliability in real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a82f6-5a99-42ae-a148-50f10737697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with new images\n",
    "import numpy as np\n",
    "\n",
    "img = tf.keras.preprocessing.image.load_img(\n",
    "    'C:\\\\Users\\\\wwwja\\\\OneDrive\\\\Desktop\\\\New folder\\\\Project\\\\LeafLens\\\\Known Data\\\\150.jpg', target_size=(299, 299)\n",
    ")\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)  # Create a batch\n",
    "\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.sigmoid(predictions[0])\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(labels[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5ac86-6281-45af-931d-55a96b15f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy and loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d1980c",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "After training the Xception model, it is essential to save the model's architecture, weights, and training configuration for future use, such as inference, further training, or deployment. This section outlines the methods for saving the model using TensorFlow/Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3163e28-47eb-4b94-b4f1-2239dae8695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('xception_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03ab06b",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Saving the model and its weights allows for efficient use of the trained model in future applications. By following the methods outlined in this section, you can ensure that your model is preserved and easily retrievable for later use, whether for inference or continued training. \n",
    "\n",
    "We will integrate this model with a camera module to enable real-time identification of leaves, enhancing its practical applications in biodiversity research and education.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
